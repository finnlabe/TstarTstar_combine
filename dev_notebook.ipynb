{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897c8821",
   "metadata": {},
   "outputs": [],
   "source": [
    "masspoint = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f256c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d02243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_histogram(pname, hin, hout, fout):\n",
    "    \"\"\"\n",
    "    get histogram from input file and write as to output file\n",
    "    \"\"\"\n",
    "    with uproot.open(pname) as fin:\n",
    "        hist = fin[hin]\n",
    "        fout[hout] = hist\n",
    "\n",
    "    return True\n",
    "\n",
    "def write_hadded_histograms(pnames, hin, hout, fout):\n",
    "    \n",
    "    with uproot.open(pnames[0]) as fin:\n",
    "        hist = fin[hin].to_hist()\n",
    "        \n",
    "    for i in range(1, len(pnames)):\n",
    "        with uproot.open(pnames[i]) as fin:\n",
    "            hist += fin[hin].to_hist()\n",
    "        \n",
    "    fout[hout] = hist\n",
    "\n",
    "def getCorrelationString(year, correlations):\n",
    "    correlationstring = \"\"\n",
    "    for entry in correlations:\n",
    "        if(year in entry):\n",
    "            for year in entry: correlationstring += year\n",
    "    if correlationstring == \"\": correlationstring + \"not_applicable\"\n",
    "        \n",
    "    return correlationstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4beb53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chars per column\n",
    "N1 = 79\n",
    "N2 = 8\n",
    "N3 = 20\n",
    "\n",
    "# just set the base path as global here\n",
    "base_path = \"/nfs/dust/cms/user/flabe/TstarTstar/data/DNN/\"\n",
    "file_prefix = \"uhh2.AnalysisModuleRunner.\"\n",
    "sensitiveVariable = \"pt_ST\"\n",
    "\n",
    "class Datacard():\n",
    "\n",
    "    def __init__(self, year, mass_point, svar, channel, region, processes):\n",
    "        self.year = year\n",
    "        self.mass_point = mass_point\n",
    "        self.svar = svar\n",
    "        self.channel = channel\n",
    "        self.region = region\n",
    "        self.fname = f\"{self.mass_point}_{self.svar}_{self.channel}_{self.region}\"\n",
    "        self.processes = processes.copy()\n",
    "        self.processes.append(\"datadriven\")\n",
    "        self.generate_datacard()\n",
    "\n",
    "    def write_block_header(self, f, block_name: str):\n",
    "        f.write(f\"# {block_name.capitalize()}\\n\")\n",
    "        f.write(N1 * \"-\" + \"\\n\")\n",
    "\n",
    "    def write_parameters(self, f):\n",
    "        self.write_block_header(f, \"parameters\")\n",
    "        f.write(f\"imax 1\\njmax {len(self.processes)-1}\\nkmax *\\n\")\n",
    "        f.write(f\"shapes * {self.region} {self.fname}.root \"\n",
    "                f\"$PROCESS $PROCESS_$SYSTEMATIC\\n\\n\")\n",
    "\n",
    "    def write_channels(self, f):\n",
    "        self.write_block_header(f, \"channels\")\n",
    "        f.write(f\"bin          {self.region}\\n\")\n",
    "        f.write(\"observation  -1\\n\\n\")\n",
    "\n",
    "    def pad(self, s, n_pad):\n",
    "        s = str(s)\n",
    "        n_pad = n_pad - len(s)\n",
    "        return s + n_pad * ' '\n",
    "\n",
    "    def write_processes(self, f):\n",
    "        padded_processes = [self.pad(x, N3) for x in self.processes]\n",
    "        padded_ids = [self.pad(i , N3) for i in range(len(self.processes))]\n",
    "        self.write_block_header(f, \"processes\")\n",
    "        f.write(self.pad(\"bin\", N1 + N2) + len(self.processes) * self.pad(self.region, N3) + \"\\n\")\n",
    "        f.write(self.pad(\"process\", N1 + N2) + \"\".join(padded_processes) + \"\\n\")\n",
    "        f.write(self.pad(\"process\", N1 + N2) + \"\".join(padded_ids) + \"\\n\")\n",
    "        f.write(self.pad(\"rate\", N1 + N2) + len(self.processes) * self.pad(\"-1\", N3))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "    def write_lnN_systematics(self, f):\n",
    "        self.write_block_header(f, \"systematics\")\n",
    "        for nuisance in norm_uncertainties:\n",
    "            f.write(self.pad(nuisance, N1) + self.pad(\"lnN\", 8))\n",
    "            for process in self.processes:\n",
    "                if process in norm_uncertainties[nuisance]:\n",
    "                    np_val = norm_uncertainties[nuisance][process]\n",
    "                    if isinstance(np_val, dict):\n",
    "                        np_val = np_val[self.year]\n",
    "                    f.write(self.pad(np_val, N3))\n",
    "                else:\n",
    "                    f.write(self.pad(\"-\", N3))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    def write_shape_systematics(self, f):\n",
    "        for shape_np in shape_uncertanties:\n",
    "            applicable_processes = shape_uncertanties[shape_np][0]\n",
    "            correlations = shape_uncertanties[shape_np][1]\n",
    "            \n",
    "            correlationstring = getCorrelationString(self.year, correlations)\n",
    "            \n",
    "            f.write(self.pad(shape_np + \"_\" + correlationstring, N1) + self.pad(\"shape\", 8))\n",
    "            \n",
    "            for process in self.processes:\n",
    "                if process in applicable_processes:\n",
    "                    f.write(self.pad(1, N3))\n",
    "                else:\n",
    "                    f.write(self.pad(\"-\", N3))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    def write_JECJER(self, f):\n",
    "        # both JEC and JER will be treated as uncorrelated between years\n",
    "        \n",
    "        correlationstring = self.year\n",
    "        f.write(self.pad(\"JEC_\" + correlationstring, N1) + self.pad(\"shape\", 8))\n",
    "        for process in self.processes:\n",
    "            if process == \"datadriven\":\n",
    "                f.write(self.pad(\"-\", N3))\n",
    "            else:\n",
    "                f.write(self.pad(1, N3))\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(self.pad(\"JER_\" + correlationstring, N1) + self.pad(\"shape\", 8))\n",
    "        for process in self.processes:\n",
    "            if process == \"datadriven\":\n",
    "                f.write(self.pad(\"-\", N3))\n",
    "            else:\n",
    "                f.write(self.pad(1, N3))\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "    def write_PDF_MCscale(self, f):\n",
    "        # these will be treated as correlated between years, but uncorrelated between samples!\n",
    "        \n",
    "        correlationstring = \"UL16UL17UL18\"\n",
    "        \n",
    "        for process in self.processes:\n",
    "            if process == \"datadriven\": continue\n",
    "            f.write(self.pad(\"PDF_\" + process +  \"_\" + correlationstring, N1) + self.pad(\"shape\", 8))\n",
    "            for process2 in self.processes:\n",
    "                if process == process2:\n",
    "                    f.write(self.pad(1, N3))\n",
    "                else:\n",
    "                    f.write(self.pad(\"-\", N3))\n",
    "            f.write(\"\\n\")\n",
    "                    \n",
    "        for process in self.processes:\n",
    "            if process == \"datadriven\": continue\n",
    "            f.write(self.pad(\"MCscale_\" + process +  \"_\" + correlationstring, N1) + self.pad(\"shape\", 8))\n",
    "            for process2 in self.processes:\n",
    "                if process == process2:\n",
    "                    f.write(self.pad(1, N3))\n",
    "                else:\n",
    "                    f.write(self.pad(\"-\", N3))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    def write_datadriven(self, f):\n",
    "        # these will be correlated through the years, and only apply to \n",
    "        \n",
    "        correlationstring = \"UL16UL17UL18\"\n",
    "        f.write(self.pad(\"datadrivenFitFunction_\" + correlationstring, N1) + self.pad(\"shape\", 8))\n",
    "        for process in self.processes:\n",
    "            if process == \"datadriven\":\n",
    "                f.write(self.pad(1, N3))\n",
    "            else:\n",
    "                f.write(self.pad(\"-\", N3))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    def generate_datacard(self):\n",
    "        with open(f\"cards/{self.year}/{self.fname}.dat\", 'w') as f:\n",
    "            self.write_parameters(f)\n",
    "            self.write_channels(f)\n",
    "            self.write_processes(f)\n",
    "            self.write_lnN_systematics(f)\n",
    "            self.write_shape_systematics(f)\n",
    "            #self.write_JECJER(f)\n",
    "            #self.write_PDF_MCscale(f)\n",
    "            self.write_datadriven(f)\n",
    "            f.write(\"* autoMCStats 10\")\n",
    "\n",
    "    def create_rootfile(self):\n",
    "        with uproot.recreate(f\"cards/{self.year}/{self.fname}.root\") as fout:\n",
    "            \n",
    "            shape_path = base_path + self.year + \"/hadded/\"\n",
    "            shape_folder = \"SignalRegion_\" + self.channel\n",
    "        \n",
    "            # moving the nominal ones\n",
    "            all_pnames = []\n",
    "            for process in self.processes:\n",
    "                if not process == \"datadriven\":\n",
    "                    hin_base = shape_folder + \"/\" + sensitiveVariable\n",
    "                    pname = shape_path + file_prefix + \"MC.\" + process + \".root\"\n",
    "                    all_pnames.append(pname)\n",
    "                    write_histogram(pname, hin_base + \"_nominal\", process, fout)\n",
    "                        \n",
    "            # data\n",
    "            hin_base = shape_folder + \"/\" + sensitiveVariable\n",
    "            pname = shape_path + file_prefix + \"DATA.DATA.root\"\n",
    "            write_histogram(pname, hin_base + \"_nominal\", \"data_obs\", fout)\n",
    "            \n",
    "            # as a fix for the moment, I'll write the total background as data!\n",
    "            # write_hadded_histograms(all_pnames, hin_base + \"_nominal\", \"data_obs\", fout)\n",
    "                    \n",
    "            # move histograms for \"normal\" shape systematics\n",
    "            for shape_np in shape_uncertanties:\n",
    "                applicable_processes = shape_uncertanties[shape_np][0]\n",
    "                correlations = shape_uncertanties[shape_np][1]\n",
    "                \n",
    "                correlationstring = getCorrelationString(self.year, correlations)\n",
    "                \n",
    "                for process in self.processes:\n",
    "                    if process in applicable_processes:\n",
    "                        pname = shape_path + file_prefix + \"MC.\" + process + \".root\"\n",
    "                        hin_base = shape_folder + \"/\" + sensitiveVariable\n",
    "                        write_histogram(pname, hin_base + \"_\" + shape_np + \"Up\",\n",
    "                                        process + \"_\" + shape_np + \"_\" + correlationstring + \"Up\", fout)\n",
    "                        write_histogram(pname, hin_base + \"_\" + shape_np + \"Down\",\n",
    "                                        process + \"_\" + shape_np + \"_\" + correlationstring + \"Down\", fout)\n",
    "\n",
    "                    \n",
    "            # JEC and JER\n",
    "            correlationstring = self.year\n",
    "            for JE in [\"JEC\", \"JER\"]:\n",
    "                for direction in [\"up\", \"down\"]:\n",
    "                    JECJER_path = base_path + self.year + \"/\" + JE + \"_\" + direction + \"/hadded/\"\n",
    "                    \n",
    "                    for process in self.processes:\n",
    "                        if not process == \"datadriven\":\n",
    "                            hin_base = shape_folder + \"/\" + sensitiveVariable\n",
    "                            pname = shape_path + file_prefix + \"MC.\" + process + \".root\"\n",
    "                            write_histogram(pname, hin_base + \"_nominal\",\n",
    "                                            process + \"_\" + JE + \"_\" + correlationstring + direction.capitalize(), fout)\n",
    "            \n",
    "                        \n",
    "            # datadriven\n",
    "            datadriven_base_path = \"/nfs/dust/cms/user/flabe/TstarTstar/data/DNN_datadriven\"\n",
    "            \n",
    "            correlationstring = self.year\n",
    "            pname = datadriven_base_path + \"/\" + self.year + \"/hadded/uhh2.AnalysisModuleRunner.DATA.datadrivenBG.root\"\n",
    "            baseline = \"SignalRegion_\" + self.channel + \"/\" + sensitiveVariable  + \"_nominal\"\n",
    "            write_histogram(pname, baseline , \"datadriven\", fout)\n",
    "            \n",
    "            # datadriven variations\n",
    "            variations = \"SR_datadrivenUp_\" + self.channel + \"/\" + sensitiveVariable  + \"_nominal\"\n",
    "            write_histogram(pname, variations , \"datadriven_datadrivenFitFunction_UL16UL17UL18Up\", fout)\n",
    "            variations = \"SR_datadrivenDown_\" + self.channel + \"/\" + sensitiveVariable  + \"_nominal\"\n",
    "            write_histogram(pname, variations , \"datadriven_datadrivenFitFunction_UL16UL17UL18Down\", fout)\n",
    "            \n",
    "            print(\"ATTENTION SKIP SOME FOR TESTS\")\n",
    "            return 1\n",
    "            \n",
    "            # PDF & scale\n",
    "            external_base_path = \"/nfs/dust/cms/user/flabe/TstarTstar/ULegacy/CMSSW_10_6_28/src/UHH2/TstarTstar/macros/rootmakros/files\"\n",
    "\n",
    "            correlationstring = \"UL16UL17UL18\"\n",
    "            for what in [\"PDF\", \"scale\"]:\n",
    "                for process in self.processes:\n",
    "                    if not process == \"datadriven\":\n",
    "                        pname = external_base_path + \"/\" + what + \"_\" + self.year + \"_\" + process + \".root\"\n",
    "                        write_histogram(pname, process + \"_\" + what +\"_up\",\n",
    "                                                process + \"_\" + what + \"_\" + correlationstring + \"Up\", fout)\n",
    "                        write_histogram(pname, process + \"_\" + what + \"_down\",\n",
    "                                                process + \"_\" + what + \"_\" + correlationstring + \"Down\", fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5e9294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, lets define a few configurations\n",
    "\n",
    "years = [\"UL16\", \"UL17\", \"UL18\"] # UL16 will be combined by hadding\n",
    "channels = [\"electron\", \"muon\"] # splitting electron and muon channel\n",
    "MC_samples = [\"TstarTstar_M-\"+str(masspoint), \"TTbar\", \"ST\"] # only top backgrounds are taken from MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce4ae2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lumi_13TeV_UL16': {'TstarTstar_M-2500': {'UL16': 1.01,\n",
       "   'UL17': '-',\n",
       "   'UL18': '-'},\n",
       "  'TTbar': {'UL16': 1.01, 'UL17': '-', 'UL18': '-'},\n",
       "  'ST': {'UL16': 1.01, 'UL17': '-', 'UL18': '-'}},\n",
       " 'lumi_13TeV_UL17': {'TstarTstar_M-2500': {'UL16': '-',\n",
       "   'UL17': 1.02,\n",
       "   'UL18': '-'},\n",
       "  'TTbar': {'UL16': '-', 'UL17': 1.02, 'UL18': '-'},\n",
       "  'ST': {'UL16': '-', 'UL17': 1.02, 'UL18': '-'}},\n",
       " 'lumi_13TeV_UL18': {'TstarTstar_M-2500': {'UL16': '-',\n",
       "   'UL17': '-',\n",
       "   'UL18': 1.015},\n",
       "  'TTbar': {'UL16': '-', 'UL17': '-', 'UL18': 1.015},\n",
       "  'ST': {'UL16': '-', 'UL17': '-', 'UL18': 1.015}},\n",
       " 'lumi_13TeV_UL16UL17UL18': {'TstarTstar_M-2500': {'UL16': 1.006,\n",
       "   'UL17': 1.009,\n",
       "   'UL18': 1.02},\n",
       "  'TTbar': {'UL16': 1.006, 'UL17': 1.009, 'UL18': 1.02},\n",
       "  'ST': {'UL16': 1.006, 'UL17': 1.009, 'UL18': 1.02}},\n",
       " 'lumi_13TeV_UL17UL18': {'TstarTstar_M-2500': {'UL16': '-',\n",
       "   'UL17': 1.006,\n",
       "   'UL18': 1.002},\n",
       "  'TTbar': {'UL16': '-', 'UL17': 1.006, 'UL18': 1.002},\n",
       "  'ST': {'UL16': '-', 'UL17': 1.006, 'UL18': 1.002}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define normalisation uncertainties for samples and years\n",
    "norm_uncertainties = {\n",
    "    \"lumi_13TeV_UL16\": {\n",
    "      p: {\"UL16\": 1.01, \"UL17\": '-', \"UL18\": '-'} for p in MC_samples\n",
    "    },\n",
    "    \"lumi_13TeV_UL17\": {\n",
    "      p: {\"UL16\": '-', \"UL17\": 1.02, \"UL18\": '-'} for p in MC_samples\n",
    "    },\n",
    "    \"lumi_13TeV_UL18\": {\n",
    "      p: {\"UL16\": '-', \"UL17\": '-', \"UL18\": 1.015} for p in MC_samples\n",
    "    },\n",
    "    \"lumi_13TeV_UL16UL17UL18\": {\n",
    "      p: {\"UL16\": 1.006, \"UL17\": 1.009, \"UL18\": 1.02} for p in MC_samples\n",
    "    },\n",
    "    \"lumi_13TeV_UL17UL18\": {\n",
    "      p: {\"UL16\": '-', \"UL17\": 1.006, \"UL18\": 1.002} for p in MC_samples\n",
    "    },\n",
    "}\n",
    "norm_uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0a6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape uncertainties that can be read as \"pt_ST_<systematic><Variation>\" where variation is \"Up\" or \"Down\"\n",
    "# these, by construction, are only relevant for the systematics taken from MC\n",
    "# planned structure: dict of variations, containing a tuple: first element defines samples, second defined corr.\n",
    "shape_uncertanties = {\n",
    "    \"pu\": [ MC_samples , [years] ], # fully correlated\n",
    "    \"prefiring\": [ MC_samples ,  [[\"UL16\"], [\"UL17\"], [\"UL18\"]] ], # uncorrelated\n",
    "    \"btagging_hf\": [ MC_samples , [years] ], # fully correlated\n",
    "    \"btagging_hfstats1\": [ MC_samples , [[\"UL16\"], [\"UL17\"], [\"UL18\"]] ], # uncorrelated\n",
    "    \"btagging_hfstats2\": [ MC_samples , [[\"UL16\"], [\"UL17\"], [\"UL18\"]] ], # uncorrelated\n",
    "    \"btagging_lf\": [ MC_samples , [years] ], # fully correlated\n",
    "    \"btagging_lfstats1\": [ MC_samples , [[\"UL16\"], [\"UL17\"], [\"UL18\"]] ], # uncorrelated\n",
    "    \"btagging_lfstats2\": [ MC_samples , [[\"UL16\"], [\"UL17\"], [\"UL18\"]] ], # uncorrelated\n",
    "    \"btagging_cferr1\": [ MC_samples , [years] ], # fully correlated\n",
    "    \"btagging_cferr2\": [ MC_samples , [years] ], # fully correlated\n",
    "    \"sfelec_id\": [ MC_samples , [years] ], # fully correlated\n",
    "    \"sfelec_reco\": [ MC_samples , [years] ], # fully correlated\n",
    "    \"sfelec_trigger\": [ MC_samples , [[\"UL16\"], [\"UL17\"], [\"UL18\"]] ], # uncorrelated\n",
    "    \"sfmu_id\": [ MC_samples , [[\"UL16\"], [\"UL17\"], [\"UL18\"]] ], # uncorrelated\n",
    "    \"sfmu_iso\": [ MC_samples , [[\"UL16\"], [\"UL17\"], [\"UL18\"]] ], # uncorrelated\n",
    "    \"sfmu_trigger\": [ MC_samples , [[\"UL16\"], [\"UL17\"], [\"UL18\"]] ], # uncorrelated\n",
    "}\n",
    "\n",
    "# additionally, we need to handle murmuf variations, and JEC/JER as well as pdfs\n",
    "# these are stored in some other files usually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f552fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacard = Datacard(years[2], masspoint, \"pt_ST\", \"mu\", \"SR\", MC_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7630d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTENTION SKIP SOME FOR TESTS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datacard.create_rootfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4afbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568ee02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21917195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_pip_setup",
   "language": "python",
   "name": "new_pip_setup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
